{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d313711-5c4d-43ce-892a-0a12a0c34296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation in progress...\n",
      "Installtion complete...\n"
     ]
    }
   ],
   "source": [
    "# Required packages, install if not installed (assume PyTorch* and IntelÂ® Extension for PyTorch* is already present)\n",
    "!echo \"Installation in progress...\"\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install  invisible-watermark > /dev/null\n",
    "# !conda install -y --quiet --prefix {sys.prefix}  -c conda-forge \\\n",
    "#     accelerate==0.23.0 \\\n",
    "#     validators==0.22.0 \\\n",
    "#     diffusers==0.18.2 \\\n",
    "#     transformers==4.32.1 \\\n",
    "#     tensorboardX \\\n",
    "#     pillow \\\n",
    "#     ipywidgets \\\n",
    "#     ipython > /dev/null && echo \"Installation successful\" || echo \"Installation failed\"\n",
    "import sys\n",
    "!{sys.executable} -m pip install invisible-watermark --user > /dev/null 2>&1 \n",
    "!{sys.executable} -m pip install transformers huggingface-hub --user > /dev/null 2>&1\n",
    "!echo \"Installtion complete...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59ba24cd-2fbd-467c-9059-d518782d7635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "# Suppress warnings for a cleaner output.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import intel_extension_for_pytorch as ipex  # adds xpu namespace to PyTorch, enabling you to use Intel GPUs\n",
    "import validators\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mp_img\n",
    "import validators\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Image as IPImage\n",
    "from ipywidgets import VBox, HBox\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import models, transforms\n",
    "from transformers import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d44effdf-08d2-4a68-bbfd-c3cf01974a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Img2ImgModel:\n",
    "    \"\"\"\n",
    "    This class creates a model for transforming images based on given prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id_or_path: str,\n",
    "        device: str = \"xpu\",\n",
    "        torch_dtype: torch.dtype = torch.bfloat16,\n",
    "        optimize: bool = True,\n",
    "        warmup: bool = False,\n",
    "        scheduler: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the model with the specified parameters.\n",
    "\n",
    "        Args:\n",
    "            model_id_or_path (str): The ID or path of the pre-trained model.\n",
    "            device (str, optional): The device to run the model on. Defaults to \"xpu\".\n",
    "            torch_dtype (torch.dtype, optional): The data type to use for the model. Defaults to torch.float16.\n",
    "            optimize (bool, optional): Whether to optimize the model. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.data_type = torch_dtype\n",
    "        self.scheduler = scheduler\n",
    "        self.generator = torch.Generator()  # .manual_seed(99)\n",
    "        self.pipeline = self._load_pipeline(model_id_or_path, torch_dtype)\n",
    "        if optimize:\n",
    "            start_time = time.time()\n",
    "            #print(\"Optimizing the model...\")\n",
    "            self.optimize_pipeline()\n",
    "            #print(\n",
    "            #    \"Optimization completed in {:.2f} seconds.\".format(\n",
    "            #        time.time() - start_time\n",
    "            #    )\n",
    "            #)\n",
    "        if warmup:\n",
    "            self.warmup_model()\n",
    "\n",
    "    def _load_pipeline(\n",
    "        self, model_id_or_path: str, torch_dtype: torch.dtype\n",
    "    ) -> StableDiffusionImg2ImgPipeline:\n",
    "        \"\"\"\n",
    "        Load the pipeline for the model.\n",
    "\n",
    "        Args:\n",
    "            model_id_or_path (str): The ID or path of the pre-trained model.\n",
    "            torch_dtype (torch.dtype): The data type to use for the model.\n",
    "\n",
    "        Returns:\n",
    "            StableDiffusionImg2ImgPipeline: The loaded pipeline.\n",
    "        \"\"\"\n",
    "        print(\"Loading the model...\")\n",
    "        model_path = Path(f\"/home/common/data/Big_Data/GenAI/{model_id_or_path}\")\n",
    "        \n",
    "        if model_path.exists():\n",
    "            #print(f\"Loading the model from {model_path}...\")\n",
    "            load_path = model_path\n",
    "        else:\n",
    "            print(\"Using the default path for models...\")\n",
    "            load_path = model_id_or_path\n",
    "            \n",
    "        pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "            load_path,\n",
    "            torch_dtype=torch_dtype,\n",
    "            use_safetensors=True,\n",
    "            variant=\"fp16\",\n",
    "        )\n",
    "        if self.scheduler:\n",
    "            pipeline.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "                pipeline.scheduler.config\n",
    "            )\n",
    "        if not model_path.exists():\n",
    "            try:\n",
    "                print(f\"Attempting to save the model to {model_path}...\")\n",
    "                pipeline.save_pretrained(f\"{model_path}\")\n",
    "                print(\"Model saved.\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while saving the model: {e}. Proceeding without saving.\")\n",
    "        pipeline = pipeline.to(self.device)\n",
    "        #print(\"Model loaded.\")\n",
    "        return pipeline\n",
    "\n",
    "    def _optimize_pipeline(\n",
    "        self, pipeline: StableDiffusionImg2ImgPipeline\n",
    "    ) -> StableDiffusionImg2ImgPipeline:\n",
    "        \"\"\"\n",
    "        Optimize the pipeline of the model.\n",
    "\n",
    "        Args:\n",
    "            pipeline (StableDiffusionImg2ImgPipeline): The pipeline to optimize.\n",
    "\n",
    "        Returns:\n",
    "            StableDiffusionImg2ImgPipeline: The optimized pipeline.\n",
    "        \"\"\"\n",
    "        for attr in dir(pipeline):\n",
    "            if isinstance(getattr(pipeline, attr), nn.Module):\n",
    "                setattr(\n",
    "                    pipeline,\n",
    "                    attr,\n",
    "                    ipex.optimize(\n",
    "                        getattr(pipeline, attr).eval(),\n",
    "                        dtype=pipeline.text_encoder.dtype,\n",
    "                        inplace=True,\n",
    "                    ),\n",
    "                )\n",
    "        return pipeline\n",
    "\n",
    "    def optimize_pipeline(self) -> None:\n",
    "        \"\"\"\n",
    "        Optimize the pipeline of the model.\n",
    "        \"\"\"\n",
    "        self.pipeline = self._optimize_pipeline(self.pipeline)\n",
    "\n",
    "    def get_image_from_url(self, url: str, path: str) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Get an image from a URL or from a local path if it exists.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL of the image.\n",
    "            path (str): The local path of the image.\n",
    "\n",
    "        Returns:\n",
    "            Image.Image: The loaded image.\n",
    "        \"\"\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Failed to download image. Status code: {response.status_code}\"\n",
    "            )\n",
    "        if not response.headers[\"content-type\"].startswith(\"image\"):\n",
    "            raise Exception(\n",
    "                f\"URL does not point to an image. Content type: {response.headers['content-type']}\"\n",
    "            )\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        img.save(path)\n",
    "        img = img.resize((768, 512))\n",
    "        return img\n",
    "\n",
    "    def warmup_model(self):\n",
    "        \"\"\"\n",
    "        Warms up the model by generating a sample image.\n",
    "        \"\"\"\n",
    "        print(\"Setting up model...\")\n",
    "        start_time = time.time()\n",
    "        image_url = \"https://user-images.githubusercontent.com/786476/256401499-f010e3f8-6f8d-4e9f-9d1f-178d3571e7b9.png\"\n",
    "        try:\n",
    "            self.generate_images(\n",
    "                image_url=image_url,\n",
    "                prompt=\"A beautiful day\",\n",
    "                num_images=1,\n",
    "                save_path=\".tmp\",\n",
    "            )\n",
    "        except Exception:\n",
    "            print(\"model warmup delayed...\")\n",
    "        #print(\n",
    "        #    \"Model is set up and ready! Warm-up completed in {:.2f} seconds.\".format(\n",
    "        #        time.time() - start_time\n",
    "        #    )\n",
    "        #)\n",
    "\n",
    "    def get_inputs(self, prompt, batch_size=1):\n",
    "        self.generator = [torch.Generator() for i in range(batch_size)]\n",
    "        prompts = batch_size * [prompt]\n",
    "        return {\"prompt\": prompts, \"generator\": self.generator}\n",
    "\n",
    "    def generate_images(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        image_url: str,\n",
    "        num_images: int = 5,\n",
    "        num_inference_steps: int = 30,\n",
    "        strength: float = 0.75,\n",
    "        guidance_scale: float = 7.5,\n",
    "        save_path: str = \"image_to_image\",\n",
    "        batch_size: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate images based on the provided prompt and variations.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The base prompt for the generation.\n",
    "            image_url (str): The URL of the seed image.\n",
    "            variations (List[str]): The list of variations to apply to the prompt.\n",
    "            num_images (int, optional): The number of images to generate. Defaults to 5.\n",
    "            num_inference_steps (int, optional): Number of noise removal steps.\n",
    "            strength (float, optional): The strength of the transformation. Defaults to 0.75.\n",
    "            guidance_scale (float, optional): The scale of the guidance. Defaults to 7.5.\n",
    "            save_path (str, optional): The path to save the generated images. Defaults to \"image_to_image\".\n",
    "\n",
    "        \"\"\"\n",
    "        input_image_path = \"input.png\"\n",
    "        init_image = self.get_image_from_url(image_url, input_image_path)\n",
    "        init_images = [init_image for _ in range(batch_size)]\n",
    "        for i in range(0, num_images, batch_size):\n",
    "            with torch.xpu.amp.autocast(\n",
    "                enabled=True if self.data_type != torch.float32 else False,\n",
    "                dtype=self.data_type,\n",
    "            ):\n",
    "                if batch_size > 1:\n",
    "                    inputs = self.get_inputs(batch_size=batch_size, prompt=prompt)\n",
    "                    images = self.pipeline(\n",
    "                        **inputs,\n",
    "                        image=init_images,\n",
    "                        strength=strength,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        num_inference_steps=num_inference_steps,\n",
    "                    ).images\n",
    "                else:\n",
    "                    images = self.pipeline(\n",
    "                        prompt=prompt,\n",
    "                        image=init_images,\n",
    "                        strength=strength,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        num_inference_steps=num_inference_steps,\n",
    "                    ).images\n",
    "\n",
    "                for j in range(len(images)):\n",
    "                    output_image_path = os.path.join(\n",
    "                        save_path,\n",
    "                        f\"{'_'.join(prompt.split()[:3])}_{i+j}__{int(time.time() * 1e6)}.png\",\n",
    "                    )\n",
    "                    images[j].save(output_image_path)\n",
    "                    \n",
    "    def generate_image(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        image: Image.Image,\n",
    "        num_images: int = 5,\n",
    "        num_inference_steps: int = 30,\n",
    "        strength: float = 0.75,\n",
    "        guidance_scale: float = 7.5,\n",
    "        save_path: str = \"image_to_image\",\n",
    "        batch_size: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate images based on the provided prompt and variations.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The base prompt for the generation.\n",
    "            image_url (str): The URL of the seed image.\n",
    "            variations (List[str]): The list of variations to apply to the prompt.\n",
    "            num_images (int, optional): The number of images to generate. Defaults to 5.\n",
    "            num_inference_steps (int, optional): Number of noise removal steps.\n",
    "            strength (float, optional): The strength of the transformation. Defaults to 0.75.\n",
    "            guidance_scale (float, optional): The scale of the guidance. Defaults to 7.5.\n",
    "            save_path (str, optional): The path to save the generated images. Defaults to \"image_to_image\".\n",
    "\n",
    "        \"\"\"\n",
    "        init_image = image\n",
    "        init_images = [init_image for _ in range(batch_size)]\n",
    "        out = []\n",
    "        for i in range(0, num_images, batch_size):\n",
    "            with torch.xpu.amp.autocast(\n",
    "                enabled=True if self.data_type != torch.float32 else False,\n",
    "                dtype=self.data_type,\n",
    "            ):\n",
    "                if batch_size > 1:\n",
    "                    inputs = self.get_inputs(batch_size=batch_size, prompt=prompt)\n",
    "                    images = self.pipeline(\n",
    "                        **inputs,\n",
    "                        image=init_images,\n",
    "                        strength=strength,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        num_inference_steps=num_inference_steps,\n",
    "                    ).images\n",
    "                else:\n",
    "                    images = self.pipeline(\n",
    "                        prompt=prompt,\n",
    "                        image=init_images,\n",
    "                        strength=strength,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        num_inference_steps=num_inference_steps,\n",
    "                    ).images\n",
    "\n",
    "                for j in range(len(images)):\n",
    "                    output_image_path = os.path.join(\n",
    "                        save_path,\n",
    "                        f\"{'_'.join(prompt.split()[:3])}_{i+j}__{int(time.time() * 1e6)}.png\",\n",
    "                    )\n",
    "                    out.append(images[j])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "model_cache = {}\n",
    "\n",
    "def image_to_image():\n",
    "    out = widgets.Output()\n",
    "    image_to_image_dir = \"image_to_image\"\n",
    "    num_images = 2\n",
    "    model_ids = [\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        \"stabilityai/stable-diffusion-2-1\",\n",
    "    ]    \n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=model_ids,\n",
    "        value=model_ids[0],\n",
    "        description=\"Model:\",\n",
    "    )    \n",
    "    prompt_text = widgets.Text(\n",
    "        value=\"\",\n",
    "        placeholder=\"Enter your prompt\",\n",
    "        description=\"Prompt:\",\n",
    "    )    \n",
    "    num_images_slider = widgets.IntSlider(\n",
    "        value=2,\n",
    "        min=1,\n",
    "        max=10,\n",
    "        step=1,\n",
    "        description=\"Images:\",\n",
    "    )    \n",
    "    image_url_text = widgets.Text(\n",
    "        value=\"https://user-images.githubusercontent.com/786476/256401499-f010e3f8-6f8d-4e9f-9d1f-178d3571e7b9.png\",\n",
    "        placeholder=\"Enter an image URL\",\n",
    "        description=\"Image URL:\",\n",
    "    )\n",
    "    enhance_checkbox = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description=\"Auto enhance the prompt?\",\n",
    "        disabled=False,\n",
    "        indent=False\n",
    "    )\n",
    "    enhance_checkbox.layout.margin = \"0 0 0 10px\"\n",
    "    num_images_slider.layout.margin = \"0 0 0 8px\"\n",
    "    prompt_text.layout.width = \"100%\"\n",
    "    layout = widgets.Layout(margin=\"0px 50px 10px 0px\")\n",
    "    button = widgets.Button(description=\"Generate Images!\", button_style=\"primary\")\n",
    "    left_box = VBox([model_dropdown,num_images_slider], layout=layout)\n",
    "    right_box = VBox([image_url_text, enhance_checkbox], layout=layout)\n",
    "    user_input_widgets = HBox([left_box, right_box], layout=layout)\n",
    "    prompt_text.layout.width = \"57.5%\"\n",
    "    button.layout.margin = \"35px\"\n",
    "    display(user_input_widgets)\n",
    "    display( prompt_text)\n",
    "    display(button)\n",
    "    display(out)\n",
    "    \n",
    "    \n",
    "    def on_submit(button):\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Once generated, images will be saved to `./image_to_image` dir, please wait...\")\n",
    "            selected_model_index = model_ids.index(model_dropdown.value)\n",
    "            model_id = model_ids[selected_model_index]\n",
    "            model_key = (model_id, \"xpu\")\n",
    "            prompt = prompt_text.value\n",
    "            num_images = num_images_slider.value\n",
    "            image_url = image_url_text.value\n",
    "            \n",
    "            if not validators.url(image_url):\n",
    "                print(\"The input is not a valid URL. Using the default URL instead.\")\n",
    "                image_url = \"https://user-images.githubusercontent.com/786476/256401499-f010e3f8-6f8d-4e9f-9d1f-178d3571e7b9.png\"       \n",
    "            #model = Img2ImgModel(model_id, device=\"xpu\")\n",
    "            if model_key not in model_cache:\n",
    "                model_cache[model_key] = Img2ImgModel(model_id, device=\"xpu\")\n",
    "            model = model_cache[model_key]\n",
    "            enhancements = [\n",
    "            \"purple light\",\n",
    "            \"dreaming\",\n",
    "            \"cyberpunk\",\n",
    "            \"ancient\" \", rustic\",\n",
    "            \"gothic\",\n",
    "            \"historical\",\n",
    "            \"punchy\",\n",
    "            \"photo\" \"vivid colors\",\n",
    "            \"4k\",\n",
    "            \"bright\",\n",
    "            \"exquisite\",\n",
    "            \"painting\",\n",
    "            \"art\",\n",
    "            \"fantasy [,/organic]\",\n",
    "            \"detailed\",\n",
    "            \"trending in artstation fantasy\",\n",
    "            \"electric\",\n",
    "            \"night\",\n",
    "            ]\n",
    "            if not prompt:\n",
    "                prompt = \" \"\n",
    "            if enhance_checkbox.value:\n",
    "                prompt = prompt + \" \" + \" \".join(random.sample(enhancements, 5))\n",
    "                print(f\"Using enhanced prompt: {prompt}\")    \n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                os.makedirs(image_to_image_dir, exist_ok=True)\n",
    "                model.generate_images(\n",
    "                    prompt=prompt,\n",
    "                    image_url=image_url,\n",
    "                    num_images=num_images,\n",
    "                )\n",
    "                clear_output(wait=True)\n",
    "                display_generated_images()\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nUser interrupted image generation...\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "            finally:\n",
    "                status = f\"Complete generating {num_images} images in {time.time() - start_time:.2f} seconds.\"\n",
    "                #print(status)\n",
    "    button.on_click(on_submit)\n",
    "\n",
    "def display_generated_images(image_to_image_dir=\"image_to_image\"):\n",
    "    image_files = [f for f in os.listdir(image_to_image_dir) if f.endswith((\".png\", \".jpg\"))]    \n",
    "    num_images = len(image_files)\n",
    "    num_columns = int(np.ceil(np.sqrt(num_images)))\n",
    "    num_rows = int(np.ceil(num_images / num_columns))\n",
    "    fig, axs = plt.subplots(num_rows, num_columns, figsize=(10 * num_columns / num_columns, 10 * num_rows / num_rows))\n",
    "    if num_images == 1:\n",
    "        axs = np.array([[axs]])\n",
    "    elif num_columns == 1 or num_rows == 1:\n",
    "        axs = np.array([axs])\n",
    "    for ax, image_file in zip(axs.ravel(), image_files):\n",
    "        img = mp_img.imread(os.path.join(image_to_image_dir, image_file))\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")  # Hide axes\n",
    "    for ax in axs.ravel()[num_images:]:\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    print(f\"\\nGenerated images...:\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "400c6ae0-b49e-4eaa-8d2a-f2ebd5d3855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf927eafde5540aab29073e4f334de50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa042a7f1974783a4893d03ae769871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8c35ab1ec1493d91da0e7bc00337ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec8af5e3aaf468e80b40be02498e98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5474505436461f828f58ce4bdc0e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3736ccb5011482ea04b8811b7a2e90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a5243d92af40a19e823c04ac367a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c72e3bc3cc44543b1639aab06421b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0556e70004c4f8d828a29731c450c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fb2ac000dc498a9a9ba3f1f85be63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_PROMPT = \"Image of a <INPUT>\"\n",
    "model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "model = Img2ImgModel(model_id, device=\"xpu\")\n",
    "\n",
    "def generate_additional_images(image, className, n=1):\n",
    "    prompt = BASE_PROMPT.replace(\"<INPUT>\", className)\n",
    "    return model.generate_image(prompt, image, n)\n",
    "\n",
    "    \n",
    "url = r\"https://firebasestorage.googleapis.com/v0/b/hacklytics-fa91a.appspot.com/o/2024-02-10T23%3A17%3A55.324Z?alt=media&token=6143c912-d0e4-4c7f-b882-2391adb653a1.\"\n",
    "img = model.get_image_from_url(url, \"img.png\")\n",
    "output = generate_additional_images(img, \"Sattelite Shot of Wildfire\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec1e53-d2aa-46df-99ba-52a85c9880e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe8a6a60-6333-406f-b7a8-ad951652f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class VGG16Settings():\n",
    "    NUM_PREDICTION_CLASSES = None\n",
    "    TRAIN_EPOCHS = 10\n",
    "    LR = .01\n",
    "    BATCH_SIZE = 32\n",
    "    DEVICE = \"xpu\"\n",
    "\n",
    "class BaseModel():\n",
    "    def __init__(settings):\n",
    "        pass\n",
    "    def train_model():\n",
    "        pass\n",
    "    def classify():\n",
    "        pass\n",
    "        \n",
    "class VGG16():\n",
    "    def __init__(self, settings : VGG16Settings):\n",
    "        # Load the pretrained VGG16 model\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "        # Freeze the convolutional layers\n",
    "        for param in vgg16.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replace the classifier\n",
    "        num_features = vgg16.classifier[0].in_features\n",
    "        vgg16.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, settings.NUM_PREDICTION_CLASSES),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        self.settings = settings\n",
    "        self.model = vgg16\n",
    "\n",
    "    def train_model(self, inputs, targets):\n",
    "        \n",
    "        #BOB's suggestion\n",
    "        train_transforms = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        # Assuming inputs are a list of PIL Images and targets are a list of labels\n",
    "        inputs_transformed = [train_transforms(input).unsqueeze(0) for input in inputs]\n",
    "        inputs_tensor = torch.cat(inputs_transformed, dim=0)\n",
    "        targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "        # Create a Dataset and DataLoader for batch processing\n",
    "        dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "        train_loader = DataLoader(dataset, batch_size=self.settings.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        # Training setup\n",
    "        self.model.to(self.settings.DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=self.settings.LR)\n",
    "\n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        for epoch in range(self.settings.TRAIN_EPOCHS):\n",
    "            total_loss = 0\n",
    "            for batch_inputs, batch_targets in train_loader:\n",
    "                batch_inputs, batch_targets = batch_inputs.to(self.settings.DEVICE), batch_targets.to(self.settings.DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_inputs)\n",
    "                loss = criterion(outputs, batch_targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss/len(train_loader)\n",
    "            print(avg_loss)\n",
    "            losses.append(avg_loss)\n",
    "        return losses\n",
    "    \n",
    "    def classify(self, image):\n",
    "        # Define the image transformation\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "        # Load and preprocess the image\n",
    "        img_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        img_tensor = img_tensor.to(self.settings.DEVICE)\n",
    "        self.model.to(self.settings.DEVICE)\n",
    "\n",
    "        # Set the model to evaluation mode and predict\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(img_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        return predicted.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "adccf843-98ff-4d7a-a90b-0544b8edd5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.32515150308609\n",
      "17.293466091156006\n",
      "5.30634880065918\n",
      "3.660771131515503\n",
      "1.883251503109932\n",
      "1.6578410863876343\n",
      "1.5726151764392853\n",
      "1.4166490137577057\n",
      "1.3383285403251648\n",
      "1.2096823304891586\n",
      "Test passed: VGG16 model instantiation, training, and classification.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming the VGG16, VGG16Settings, and BaseModel classes are defined as provided\n",
    "\n",
    "# Step 1: Define a test case function\n",
    "def test_vgg16():\n",
    "    # Setup\n",
    "    settings = VGG16Settings()\n",
    "    settings.NUM_PREDICTION_CLASSES = 10  # Example: 10 classes for classification\n",
    "    settings.DEVICE = \"cpu\" #\"cuda\" if torch.cuda.is_available() else \"cpu\"  # MKL makes this faster than XPU\n",
    "    \n",
    "    # Instantiate the VGG16 model\n",
    "    model = VGG16(settings=settings)\n",
    "    \n",
    "    # Generate a dummy dataset\n",
    "    # For training: Create 100 dummy images (3x224x224) and labels\n",
    "    num_samples = 100\n",
    "    dummy_images = [Image.fromarray(np.uint8(np.random.rand(224, 224, 3) * 255)) for _ in range(num_samples)]\n",
    "    dummy_labels = np.random.randint(0, settings.NUM_PREDICTION_CLASSES, num_samples)\n",
    "    \n",
    "    # Train the model with the dummy dataset\n",
    "    model.train_model(dummy_images, dummy_labels)\n",
    "    \n",
    "    # Generate a single dummy image for classification\n",
    "    test_image = Image.fromarray(np.uint8(np.random.rand(224, 224, 3) * 255))\n",
    "    \n",
    "    # Classify the image\n",
    "    prediction = model.classify(test_image)\n",
    "    assert isinstance(prediction, int), \"The classification method should return an integer\"\n",
    "    assert 0 <= prediction < settings.NUM_PREDICTION_CLASSES, \"The prediction should be within the range of possible classes\"\n",
    "    print(\"Test passed: VGG16 model instantiation, training, and classification.\")\n",
    "\n",
    "# Step 2: Run the test\n",
    "test_vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e6255ad6-0ff8-455f-b29b-b632a2338208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class ViTSettings():\n",
    "    NUM_PREDICTION_CLASSES = 1000\n",
    "    TRAIN_EPOCHS = 10\n",
    "    LR = .01\n",
    "    BATCH_SIZE = 32\n",
    "    DEVICE = \"xpu\"\n",
    "\n",
    "class ViTModel():\n",
    "    def __init__(self, settings : ViTSettings):\n",
    "        # Load the pretrained ViT model and feature extractor\n",
    "        self.model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.model.classifier = nn.Linear(self.model.config.hidden_size, settings.NUM_PREDICTION_CLASSES)\n",
    "        self.model = ipex.optimize(self.model)\n",
    "        self.feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.settings = settings\n",
    "\n",
    "    def train_model(self, inputs, targets):\n",
    "        # Define the image transformations\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "        # Apply transformations to each image in inputs\n",
    "        inputs_transformed = [transform(input_image) for input_image in inputs]\n",
    "        inputs_tensor = torch.stack(inputs_transformed)\n",
    "        targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "        # Create a Dataset and DataLoader for batch processing\n",
    "        dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "        train_loader = DataLoader(dataset, batch_size=self.settings.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        # Training setup\n",
    "        self.model.to(self.settings.DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=self.settings.LR)\n",
    "\n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        for epoch in range(self.settings.TRAIN_EPOCHS):\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                batch_inputs, batch_targets = batch[0].to(self.settings.DEVICE), batch[1].to(self.settings.DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(pixel_values=batch_inputs).logits\n",
    "\n",
    "                loss = criterion(outputs, batch_targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}, Average Loss: {avg_loss}\")\n",
    "            losses.append(avg_loss)\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def classify(self, image):\n",
    "        # Define the image transformation\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "        # Load and preprocess the image\n",
    "        img_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "        img_tensor = img_tensor.to(self.settings.DEVICE)\n",
    "        self.model.to(self.settings.DEVICE)\n",
    "    \n",
    "        # Set the model to evaluation mode and predict\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(pixel_values=img_tensor).logits\n",
    "        predicted_class_idx = outputs.argmax(-1).item()\n",
    "    \n",
    "        return predicted_class_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5c8da6e3-d1cc-4a0b-b11e-96c5f67f8061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 2.553674876689911\n",
      "Epoch 2, Average Loss: 2.4934158325195312\n",
      "Epoch 3, Average Loss: 2.6183581948280334\n",
      "Epoch 4, Average Loss: 2.5135324597358704\n",
      "Epoch 5, Average Loss: 2.5134425163269043\n",
      "Epoch 6, Average Loss: 2.506541073322296\n",
      "Epoch 7, Average Loss: 2.544667661190033\n",
      "Epoch 8, Average Loss: 2.463534951210022\n",
      "Epoch 9, Average Loss: 2.607055187225342\n",
      "Epoch 10, Average Loss: 2.5501466393470764\n",
      "Predicted class index: 9\n",
      "Test passed: CustomViTModel model instantiation, training, and classification.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming the CustomViTModel and CustomViTSettings classes are defined as provided\n",
    "\n",
    "# Step 1: Define a test case function\n",
    "def test_vit_model():\n",
    "    # Setup\n",
    "    settings = ViTSettings()\n",
    "    settings.NUM_PREDICTION_CLASSES = 10  # Example: Adjust based on your actual classes\n",
    "    settings.DEVICE = \"cpu\" #\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Instantiate the ViT model\n",
    "    model = ViTModel(settings=settings)\n",
    "    \n",
    "    # Generate a dummy dataset\n",
    "    # For training: Create dummy images (3x224x224) and labels\n",
    "    num_samples = 100\n",
    "    dummy_images = [Image.fromarray(np.uint8(np.random.rand(224, 224, 3) * 255)) for _ in range(num_samples)]\n",
    "    dummy_labels = np.random.randint(0, settings.NUM_PREDICTION_CLASSES, num_samples)\n",
    "    \n",
    "    # Train the model with the dummy dataset\n",
    "    losses = model.train_model(dummy_images, dummy_labels)\n",
    "    assert losses, \"Training should produce a list of losses\"\n",
    "    \n",
    "    # Generate a single dummy image for classification\n",
    "    test_image = Image.fromarray(np.uint8(np.random.rand(224, 224, 3) * 255))\n",
    "    \n",
    "    # Classify the image\n",
    "    predicted_class_idx = model.classify(test_image)\n",
    "    print(f\"Predicted class index: {predicted_class_idx}\")\n",
    "    assert isinstance(predicted_class_idx, int), \"The classification method should return an integer\"\n",
    "    assert 0 <= predicted_class_idx < settings.NUM_PREDICTION_CLASSES, \"The predicted class index should be within the range of possible classes\"\n",
    "    \n",
    "    print(\"Test passed: CustomViTModel model instantiation, training, and classification.\")\n",
    "\n",
    "# Step 2: Run the test\n",
    "test_vit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa00646-a69c-40a1-b947-28c01ea3fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServerOperationHandler():\n",
    "    def __init__(self):\n",
    "        self.CONNECTOR = None\n",
    "        self.models = dict()\n",
    "        self.url = \"http://18.188.69.104:5000\"\n",
    "\n",
    "    def main_loop(self):\n",
    "        while(True):\n",
    "            self.get_compute_job()\n",
    "            self.get_classify_jobs()\n",
    "            time.sleep(0.5)\n",
    "        pass # Keep calling this to get jobs and process them\n",
    "\n",
    "    def get_compute_job(self):\n",
    "        response = requests.get(self.url + \"/checkModels\")\n",
    "        if (response.status_code is 200):\n",
    "            data = response.json()\n",
    "            imageData = data[\"data\"][\"imageData\"]\n",
    "            images = [model.get_image_from_url(image[\"imageUrl\"]) for image in imageData]\n",
    "            categories = [image[\"category\"] for image in imageData]\n",
    "            if data[\"data\"][\"settings\"][\"modelType\"] is \"CNN\":\n",
    "                self.process_train_job(VGG16, data[\"id\"], VGG16Settings, images, categories )\n",
    "            else:\n",
    "                self.process_train_job(ViTModel, data[\"id\"], ViTSettings, images, categories)\n",
    "        pass\n",
    "        \n",
    "    def get_classify_jobs(self):\n",
    "        response = requests.get(self.url + \"/checkJobs\")\n",
    "        if (response.status_code is 200):\n",
    "            data = response.json()\n",
    "            self.process_classify_job(data[\"data\"][\"model\"], data[\"id\"], data[\"data\"][\"imageUrl\"])\n",
    "        pass\n",
    "\n",
    "    def process_train_job(self, model, modelid, settings, inputs, targets): #Initalized Model Input\n",
    "        self.models[modelid] = model(settings)\n",
    "        self.train_model(inputs, targets)\n",
    "        requests.post(self.url + '/finishModel', json={\"id\": modelid})\n",
    "\n",
    "    def process_classify_job(self, modelid, jobid, input):\n",
    "        output = self.models[modelid].classify(input)\n",
    "        requests.post(self.url + '/finishJob', json={\"id\": jobid, \"output\": output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49682a-4cee-4b09-9fc8-961a5207f1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
